{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SparkContext is already defined as sc\n",
    "HDFS = 'hdfs://ScutAmazon:9000/ml-100k/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploer Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### user file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1|24|M|technician|85711\n",
      "Lines Count:943\n",
      "users: 943, genders: 2, occupations: 21, zipcodes: 795\n",
      "[10  8  3  2  7  1 16 11 12  5 18 14  9 13 17  6 20  0 15 19  4]\n"
     ]
    }
   ],
   "source": [
    "# user file\n",
    "f = 'u.user'\n",
    "user_data = sc.textFile(HDFS+f)\n",
    "print user_data.first()\n",
    "print 'Lines Count:%s'%user_data.count()\n",
    "user_fields = user_data.map(lambda line: line.split('|'))\n",
    "num_users = user_fields.map(lambda fields: fields[0]).distinct().count()\n",
    "num_genders = user_fields.map(lambda fields: fields[2]).distinct().count()\n",
    "num_occupations = user_fields.map(lambda fields: fields[3]).distinct().count()\n",
    "num_zipcodes = user_fields.map(lambda fields: fields[4]).distinct().count()\n",
    "print 'users: %s, genders: %s, occupations: %s, zipcodes: %s'%(num_users, num_genders, num_occupations, num_zipcodes)\n",
    "\n",
    "comment = \"\"\"cannot draw in pyspark\n",
    "from matplotlib import pyplot\n",
    "ages = user_fields.map(lambda fields: int(fields[1])).collect()\n",
    "pyplot.hist(ages, bins = 20, normed = True)\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "fig.set_size_inches(16, 10)\n",
    "\"\"\"\n",
    "\n",
    "count_by_occupation = user_fields.map(lambda x : (x[3], 1))\n",
    "count_by_occupation = count_by_occupation.reduceByKey(lambda a,b:a+b).collect()\n",
    "# equal to the above two lines\n",
    "# count_by_occupation = user_fields.map(lambda x : x[3]).countByValue()\n",
    "\n",
    "# cannot draw\n",
    "import numpy as np\n",
    "x_axis = np.array([c[0] for c in count_by_occupation])\n",
    "y_axis = np.array([c[1] for c in count_by_occupation])\n",
    "\n",
    "print np.argsort(y_axis) # return indices of elements in the list after being sorted\n",
    "x_axis = x_axis[np.argsort(y_axis)]\n",
    "y_axis = x_axis[np.argsort(y_axis)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### movie file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1|Toy Story (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Toy%20Story%20(1995)|0|0|0|1|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0\n",
      "1682\n",
      "[1922, 1926, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998]\n"
     ]
    }
   ],
   "source": [
    "f = 'u.item'\n",
    "movie_data = sc.textFile(HDFS+f)\n",
    "print movie_data.first()\n",
    "num_movies = movie_data.count()\n",
    "print num_movies\n",
    "\n",
    "def convert_year(x):\n",
    "    try:\n",
    "        return int(x[-4:])\n",
    "    except:\n",
    "        return 1990\n",
    "movie_fields = movie_data.map(lambda x:x.split('|'))\n",
    "years = movie_fields.map(lambda fields:fields[2]).map(lambda x:convert_year(x))\n",
    "count_by_year = years.countByValue()\n",
    "print count_by_year.keys()\n",
    "\n",
    "comment = \"\"\"draw the histogrm\n",
    "from matplotlib import pyplot\n",
    "pyplot.hist(count_by_year, bins = count_by_years.keys(), normed = True)\n",
    "fig = pyplot.gvf()\n",
    "fig.set_size_inches(16, 10)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rating file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196\t242\t3\t881250949\n",
      "Rating count:100000\n",
      "max rating: 5\n",
      "min rating: 1\n",
      "average rating: 3.53\n",
      "median rating: 4.0\n",
      "average rating: 106\n",
      "rating per movie: 59\n",
      "(count: 100000, mean: 3.52986, stdev: 1.12566797076, max: 5.0, min: 1.0)\n",
      "[(u'344', 190), (u'346', 193), (u'340', 44), (u'342', 201), (u'810', 26), (u'812', 20), (u'814', 35), (u'816', 25), (u'719', 67), (u'717', 93)]\n"
     ]
    }
   ],
   "source": [
    "f = 'u.data'\n",
    "rating_data = sc.textFile(HDFS+f)\n",
    "print rating_data.first()\n",
    "num_ratings = rating_data.count()\n",
    "print \"Rating count:%s\"%num_ratings\n",
    "\n",
    "rating_fields = rating_data.map(lambda x:x.split('\\t'))\n",
    "rating = rating_fields.map(lambda x:int(x[2]))\n",
    "max_rating = rating.reduce(lambda a,b: max(a,b))\n",
    "print 'max rating: %s'%max_rating\n",
    "min_rating = rating.reduce(lambda a,b: min(a,b))\n",
    "print 'min rating: %s'%min_rating\n",
    "mean_rating = rating.reduce(lambda a,b: a+b)*1.0/num_ratings\n",
    "print 'average rating: %2.2f'%mean_rating\n",
    "median_rating = np.median(rating.collect())\n",
    "print 'median rating: %s'%median_rating\n",
    "average_rating = num_ratings/num_users\n",
    "print 'average rating: %s'%average_rating\n",
    "rating_per_movie = num_ratings/num_movies\n",
    "print 'rating per movie: %s'%rating_per_movie\n",
    "\n",
    "# build-in function\n",
    "print rating.stats()\n",
    "\n",
    "user_ratings = rating_fields.map(lambda fields: (fields[0], fields[2])).groupByKey()\n",
    "user_ratings_by_user = user_ratings.map(lambda (k, v): (k, len(v)))\n",
    "print user_ratings_by_user.take(10)\n",
    "\n",
    "comment =\"\"\"draw the histgram\n",
    "user_ratings_by_user_count = user_ratings_by_user.map(lambda (k,v) : v)\n",
    "pyplot.hist(user_ratings_by_user_count, bins = user_ratings_by_user_count.count(), normed = True)\n",
    "fig = pyplot.gcf()\n",
    "fig.set_size_inches(16, 10)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1989.38644471\n",
      "mean of the years: 1989.37718769\n",
      "median of the years: 1995.0\n",
      "all missing years are filled with median year\n"
     ]
    }
   ],
   "source": [
    "print years.mean() # don't need collect() to return all the data to driven program\n",
    "years_array = np.array(years.collect())\n",
    "mean_year = np.mean(years_array[years_array != 1990])\n",
    "median_year = np.median(years_array[years_array != 1990])\n",
    "print 'mean of the years: %s'%mean_year\n",
    "print 'median of the years: %s'%median_year\n",
    "\n",
    "bad_indcices = np.where(years_array[years_array == 1990])[0][0]\n",
    "years_array[bad_indcices] = median_year\n",
    "print 'all missing years are filled with median year'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Process and Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### category feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one hot encoding for programmer is \n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# one-hot encoding\n",
    "import numpy as np\n",
    "all_occupation = user_fields.map(lambda fields:fields[3]).distinct().collect()\n",
    "all_occupation.sort()\n",
    "mapping = {}\n",
    "for i in xrange(len(all_occupation)):\n",
    "    mapping[all_occupation[i]] = i\n",
    "k = len(all_occupation)\n",
    "occupation = 'programmer'\n",
    "one_hot_vec = np.zeros(k)\n",
    "one_hot_vec[mapping[occupation]] = 1\n",
    "print 'one hot encoding for %s is \\n %s'%(occupation, one_hot_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### derived features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23, 3, 15, 13, 13]\n",
      "['evening', 'night', 'afternoon', 'lunch', 'lunch']\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "def extract_hour(timestamp):\n",
    "    \"\"\"extract hour from unix time stamp, can also extract year, month, day, miniute\"\"\"\n",
    "    return datetime.datetime.fromtimestamp(timestamp).hour\n",
    "\n",
    "timestamps = rating_fields.map(lambda fields:int(fields[3]))\n",
    "hours = timestamps.map(extract_hour)\n",
    "print hours.take(5)\n",
    "\n",
    "def assign_tag(hour):\n",
    "    mapping = {'morning': range(6,12), 'lunch': range(12, 14), 'afternoon': range(14, 18), 'evening': range(18, 24), 'night':range(0, 6)}\n",
    "    for k, v in mapping.items():\n",
    "        if hour in v:\n",
    "            return k\n",
    "\n",
    "tags = hours.map(assign_tag)\n",
    "print tags.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Toy Story', u'GoldenEye', u'Four Rooms', u'Get Shorty', u'Copycat']\n",
      "[[u'Toy', u'Story'], [u'GoldenEye'], [u'Four', u'Rooms'], [u'Get', u'Shorty'], [u'Copycat']]\n",
      "Total number of terms: 2645\n",
      "Index of term 'Dead': 146\n",
      "Index of term 'Rooms': 1963\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<1x2645 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 2 stored elements in Compressed Sparse Column format>, <1x2645 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 1 stored elements in Compressed Sparse Column format>, <1x2645 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 2 stored elements in Compressed Sparse Column format>, <1x2645 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 2 stored elements in Compressed Sparse Column format>, <1x2645 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 1 stored elements in Compressed Sparse Column format>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def extract_title(raw):\n",
    "    # this regular expression finds the non-word (numbers) between parentheses\n",
    "    grps = re.search(\"\\((\\w+)\\)\", raw)\n",
    "    if grps: # we take only the title part, and strip the trailing whitespace from the remaining text, below\n",
    "        return raw[:grps.start()].strip()\n",
    "    else:\n",
    "        return raw\n",
    "\n",
    "# step1: extract text\n",
    "raw_titles = movie_fields.map(lambda fields: fields[1])\n",
    "titles = raw_titles.map(extract_title)\n",
    "print titles.take(5)\n",
    "\n",
    "# step2: tokenizer\n",
    "title_terms = titles.map(lambda t: t.split())\n",
    "print title_terms.take(5)\n",
    "\n",
    "# step3: build dictionary to map word to integer\n",
    "all_terms = title_terms.flatMap(lambda x: x).distinct().collect()\n",
    "idx = 0\n",
    "all_terms_dict = {}\n",
    "for term in all_terms:\n",
    "    all_terms_dict[term] = idx\n",
    "    idx += 1\n",
    "\n",
    "print \"Total number of terms: %d\" % len(all_terms_dict)\n",
    "print \"Index of term 'Dead': %d\" % all_terms_dict['Dead']\n",
    "print \"Index of term 'Rooms': %d\" % all_terms_dict['Rooms']\n",
    "\n",
    "# the above can also be implemented in paralle \n",
    "all_terms_dict1 = title_terms.flatMap(lambda x: x).distinct().zipWithIndex().collectAsMap()\n",
    "# print all_terms_dict1\n",
    "\n",
    "# step4: convert a list of terms to a vector\n",
    "from scipy import sparse as sp\n",
    "def create_vector(terms, term_dict):\n",
    "    n = len(term_dict)\n",
    "    vector = sp.csc_matrix((1, n))\n",
    "    for term in terms:\n",
    "        if term in term_dict:\n",
    "            vector[0, term_dict[term]] = 1\n",
    "    return vector\n",
    "\n",
    "# broadcast variable\n",
    "all_terms_broad = sc.broadcast(all_terms_dict)\n",
    "vectors = title_terms.map(lambda terms: create_vector(terms, all_terms_broad.value))\n",
    "vectors.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normalize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================normalize with numpy=======================\n",
      "x:\n",
      "[ 0.49671415 -0.1382643   0.64768854  1.52302986 -0.23415337 -0.23413696\n",
      "  1.57921282  0.76743473 -0.46947439  0.54256004]\n",
      "2-Norm of x: 2.5908\n",
      "Normalized x:\n",
      "[ 0.19172213 -0.05336737  0.24999534  0.58786029 -0.09037871 -0.09037237\n",
      "  0.60954584  0.29621508 -0.1812081   0.20941776]\n",
      "2-Norm of normalized_x: 1.0000\n",
      "===================normalize with spark=======================\n",
      "x:\n",
      "[ 0.49671415 -0.1382643   0.64768854  1.52302986 -0.23415337 -0.23413696\n",
      "  1.57921282  0.76743473 -0.46947439  0.54256004]\n",
      "Normalized x MLLib:\n",
      "[ 0.19172213 -0.05336737  0.24999534  0.58786029 -0.09037871 -0.09037237\n",
      "  0.60954584  0.29621508 -0.1812081   0.20941776]\n",
      "2-Norm of normalized_x_mllib: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# normalize with numpy\n",
    "print '===================normalize with numpy======================='\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "x = np.random.randn(10)\n",
    "norm_2 = np.linalg.norm(x)\n",
    "normalized_x = x/norm_2\n",
    "print \"x:\\n%s\" % x\n",
    "print \"2-Norm of x: %2.4f\" % norm_2\n",
    "print \"Normalized x:\\n%s\" % normalized_x\n",
    "print \"2-Norm of normalized_x: %2.4f\" %np.linalg.norm(normalized_x)\n",
    "\n",
    "\n",
    "print '===================normalize with spark======================='\n",
    "from pyspark.mllib.feature import Normalizer\n",
    "normalizer = Normalizer()\n",
    "vector = sc.parallelize([x])\n",
    "normalized_x_mllib = normalizer.transform(vector).first().toArray()\n",
    "print \"x:\\n%s\" % x\n",
    "print \"Normalized x MLLib:\\n%s\" % normalized_x_mllib\n",
    "print \"2-Norm of normalized_x_mllib: %2.4f\" %np.linalg.norm(normalized_x_mllib)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Apache Toree - PySpark",
   "language": "python",
   "name": "apache_toree_pyspark"
  },
  "language_info": {
   "codemirror_mode": "text/x-ipython",
   "file_extension": ".py",
   "mimetype": "text/x-ipython",
   "name": "python",
   "pygments_lexer": "python",
   "version": "2.7.12\n"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
