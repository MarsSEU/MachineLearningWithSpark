{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SparkContext is already defined as sc\n",
    "HDFS = 'hdfs://ScutAmazon:9000/ml-100k/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model with explict data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:0.00806581210266\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
    "\n",
    "# avoid the case when iteration is large, stackoverflow error occurs\n",
    "# see, https://stackoverflow.com/questions/31484460/spark-gives-a-stackoverflowerror-when-training-using-als\n",
    "sc.setCheckpointDir('hdfs://ScutAmazon:9000/checkpoint')\n",
    "\n",
    "# extract rating data\n",
    "data = sc.textFile(HDFS+'u.data')\n",
    "data_fileds = data.map(lambda line: line.split())\n",
    "ratings = data_fileds.map(lambda fields: Rating(fields[0], fields[1], fields[2]))\n",
    "\n",
    "# parameters\n",
    "rank = 200\n",
    "iterations = 50\n",
    "lambda_ = 0.01\n",
    "\n",
    "# train model and validate with MSE\n",
    "# referer: https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.recommendation.ALS\n",
    "model = ALS.train(ratings, rank, iterations, lambda_)\n",
    "test_data = ratings.map(lambda r: (r[0], r[1]))\n",
    "predictions = model.predictAll(test_data).map(lambda r: ((r[0], r[1]), r[2])) # predictAll return a list of Rating\n",
    "true_ratings = ratings.map(lambda r: ((int(r[0]), int(r[1])), float(r[2])))\n",
    "# join is called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs \n",
    "ratings_and_predictions = predictions.join(true_ratings) \n",
    "mes = ratings_and_predictions.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "print 'MSE:{0}'.format(mes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Use the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## top k recommendation for a user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====top 10 recommendataion for user 789======\n",
      "Rating(user=789, product=127, rating=4.975318665475989)\n",
      "Rating(user=789, product=100, rating=4.973055095155739)\n",
      "Rating(user=789, product=475, rating=4.9693345410273295)\n",
      "Rating(user=789, product=276, rating=4.9691825418727475)\n",
      "Rating(user=789, product=129, rating=4.961758935939646)\n",
      "Rating(user=789, product=150, rating=4.960734391882507)\n",
      "Rating(user=789, product=9, rating=4.955455781631823)\n",
      "Rating(user=789, product=50, rating=4.955384189847999)\n",
      "Rating(user=789, product=741, rating=4.929019832825062)\n",
      "Rating(user=789, product=663, rating=4.88606841224948)\n"
     ]
    }
   ],
   "source": [
    "K = 10\n",
    "user_id = 789\n",
    "top_k = model.recommendProducts(user_id, K)\n",
    "print '=====top {1} recommendataion for user {0}======'.format(user_id, K)\n",
    "for item in top_k:\n",
    "    print item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1|Toy Story (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Toy%20Story%20(1995)|0|0|0|1|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0\n",
      "Citizen Kane (1941)\n",
      "=========10 most favorite rated movies for user 789===========\n",
      "Godfather, The (1972) 5.0\n",
      "Trainspotting (1996) 5.0\n",
      "Dead Man Walking (1995) 5.0\n",
      "Star Wars (1977) 5.0\n",
      "Swingers (1996) 5.0\n",
      "Leaving Las Vegas (1995) 5.0\n",
      "Bound (1996) 5.0\n",
      "Fargo (1996) 5.0\n",
      "Last Supper, The (1995) 5.0\n",
      "Private Parts (1997) 4.0\n",
      "=====top 10 recommendataion for user 789======\n",
      "Godfather, The (1972) 4.97531866548\n",
      "Fargo (1996) 4.97305509516\n",
      "Trainspotting (1996) 4.96933454103\n",
      "Leaving Las Vegas (1995) 4.96918254187\n",
      "Bound (1996) 4.96175893594\n",
      "Swingers (1996) 4.96073439188\n",
      "Dead Man Walking (1995) 4.95545578163\n",
      "Star Wars (1977) 4.95538418985\n",
      "Last Supper, The (1995) 4.92901983283\n",
      "Being There (1979) 4.88606841225\n"
     ]
    }
   ],
   "source": [
    "# vertify the top k recommendation from their content\n",
    "movies = sc.textFile(HDFS + 'u.item')\n",
    "print movies.first()\n",
    "# mapping of movie id and its title\n",
    "titles = movies.map(lambda line: line.split('|')[:2]).map(lambda t: (int(t[0]), t[1])).collectAsMap()\n",
    "print titles[134]\n",
    "\n",
    "# find the most favorite movies that are already rated by user_id\n",
    "\n",
    "# rated_movies = ratings.keyBy(lambda r: int(r.user)).lookup(user_id) # return a list rather than a rdd\n",
    "rated_movies = ratings.filter(lambda r: int(r.user) == user_id).sortBy(lambda r: r.rating, ascending = False).take(10)\n",
    "print '========={0} most favorite rated movies for user {1}==========='.format(len(rated_movies), user_id)\n",
    "for r in rated_movies:\n",
    "    print titles[r.product], r.rating\n",
    "\n",
    "print '=====top {0} recommendataion for user {1}======'.format(K, user_id)\n",
    "for r in top_k:\n",
    "    print titles[r.product], r.rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## similar item recommendataion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============10 most similar movies to Wes Craven's New Nightmare (1994)==========\n",
      "Wes Craven's New Nightmare (1994) 1.0\n",
      "Body Snatchers (1993) 0.67708296453\n",
      "Tales from the Crypt Presents: Bordello of Blood (1996) 0.66785730695\n",
      "Stephen King's The Langoliers (1995) 0.659703025353\n",
      "Braindead (1992) 0.656651008989\n",
      "Blink (1994) 0.635719989761\n",
      "Albino Alligator (1996) 0.633008686004\n",
      "Nightmare on Elm Street, A (1984) 0.628060514577\n",
      "Paradise Lost: The Child Murders at Robin Hood Hills (1996) 0.613623292345\n",
      "Edge, The (1997) 0.611766252378\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def cosineSimilarity(vec1, vec2):\n",
    "    \"\"\"vec1 and vec2 are numpy array\"\"\"\n",
    "    return (vec1.dot(vec2.T)/(np.linalg.norm(vec1) * np.linalg.norm(vec2)))[0]\n",
    "item_id = 567\n",
    "# has the item_id as key\n",
    "item_factor = model.productFeatures().filter(lambda i: i[0] == item_id)\n",
    "#print item_factor.collect()\n",
    "# don't have item_id as key\n",
    "item_factor = model.productFeatures().lookup(item_id)\n",
    "item_factor =  np.array(item_factor)\n",
    "\n",
    "sims = model.productFeatures().map(lambda p: (int(p[0]), cosineSimilarity(item_factor, np.array(p[1]))))\n",
    "most_sims = sims.sortBy(lambda p: p[1], ascending = False).take(10)\n",
    "print '============{0} most similar movies to {1}=========='.format(len(most_sims), titles[item_id])\n",
    "for p in most_sims:\n",
    "    # print p\n",
    "    print titles[p[0]], p[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate performance of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Mean Sqared Error(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:0.00806581210266\n"
     ]
    }
   ],
   "source": [
    "# same as above\n",
    "test_data = ratings.map(lambda r: (r[0], r[1]))\n",
    "predictions = model.predictAll(test_data).map(lambda r: ((r[0], r[1]), r[2])) # predictAll return a list of Rating\n",
    "true_ratings = ratings.map(lambda r: ((int(r[0]), int(r[1])), float(r[2])))\n",
    "# join is called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs \n",
    "predictions_and_actual = predictions.join(true_ratings)\n",
    "mes = predictions_and_actual.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "print 'MSE:{0}'.format(mes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean average precision at K(MAPK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APK for user 212 : 0.9\n",
      "MAPK for all users : 0.295360551432\n"
     ]
    }
   ],
   "source": [
    "# definition: https://www.kaggle.com/wiki/MeanAveragePrecision\n",
    "def avgPrecisionK(actual_ratings, predicted_ratings, k):\n",
    "    \"\"\"compute APK for a user\n",
    "    actual_ratings: list of Ratings\n",
    "    predicted_ratings: list of Ratings\n",
    "    \"\"\"\n",
    "    actual = [r.product for r in actual_ratings]\n",
    "    predicted = [r.product for r in predicted_ratings]\n",
    "    predicted_k = predicted[:k]\n",
    "    score, hits = 0, 0\n",
    "    for i in xrange(len(predicted_k)):\n",
    "        if predicted_k[i] in actual:\n",
    "            hits += 1\n",
    "            score += 1.0*hits/(i+1)\n",
    "    if len(actual) == 0 or len(predicted_k) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return score/min(len(actual), len(predicted_k))\n",
    "\n",
    "# test for one user\n",
    "print 'APK for user {0} : {1}'.format(user_id, avgPrecisionK(rated_movies, top_k, 10))\n",
    "\n",
    "\n",
    "\n",
    "# compute APK for each user and average them to get MAPK\n",
    "K = 10\n",
    "# get all users\n",
    "total_apk = 0\n",
    "user_ids = sc.textFile(HDFS+'u.user').map(lambda line : int(line.split('|')[0])).collect()\n",
    "for user_id in user_ids:\n",
    "    total_apk += avgPrecisionK(ratings.filter(lambda r: int(r.user) == user_id).sortBy(lambda r: r.rating, ascending = False).take(10),\\\n",
    "                        model.recommendProducts(user_id, K), K)\n",
    "MAPK = 1.0 * total_apk/len(user_ids)\n",
    "print 'MAPK for all users : {0}'.format(MAPK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLlib's built-in evaluation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSE and MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:0.0898098663993, MSE:0.00806581210266\n"
     ]
    }
   ],
   "source": [
    "# https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.evaluation.RegressionMetrics\n",
    "from  pyspark.mllib.evaluation import RegressionMetrics\n",
    "predict_and_true = predictions_and_actual.map(lambda pr:pr[1])\n",
    "metrics = RegressionMetrics(predict_and_true)\n",
    "print 'RMSE:{0}, MSE:{1}'.format(metrics.rootMeanSquaredError, metrics.meanSquaredError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " consuming time: 1196.98456383s, MAPK: 0.294608771398\n"
     ]
    }
   ],
   "source": [
    "# https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.evaluation.RankingMetrics\n",
    "from pyspark.mllib.evaluation import RankingMetrics\n",
    "import time\n",
    "K = 10\n",
    "\n",
    "start_time = time.time()\n",
    "recommend, true = [], []\n",
    "user_ids = sc.textFile(HDFS+'u.user').map(lambda line : int(line.split('|')[0])).collect()\n",
    "for user_id in user_ids:\n",
    "    r_ratings = model.recommendProducts(user_id, K)\n",
    "    t_ratings = ratings.filter(lambda r: int(r.user) == user_id).sortBy(lambda r: r.rating, ascending = False).take(K)\n",
    "    recommend.append([r.product for r in r_ratings])\n",
    "    true.append([r.product for r in t_ratings])\n",
    "metrics = RankingMetrics(sc.parallelize(zip(recommend, true)))\n",
    "print 'consuming time: {0}s, MAPK: {1}'.format(time.time() - start_time, metrics.meanAveragePrecision)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Apache Toree - PySpark",
   "language": "python",
   "name": "apache_toree_pyspark"
  },
  "language_info": {
   "codemirror_mode": "text/x-ipython",
   "file_extension": ".py",
   "mimetype": "text/x-ipython",
   "name": "python",
   "pygments_lexer": "python",
   "version": "2.7.12\n"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
