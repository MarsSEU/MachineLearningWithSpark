{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SparkContext is already defined as sc\n",
    "HDFS = 'hdfs://scut0:9000/bike/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the right features from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17379\n",
      "[u'1', u'2011-01-01', u'1', u'0', u'1', u'0', u'0', u'6', u'0', u'1', u'0.24', u'0.2879', u'0.81', u'0', u'3', u'13', u'16']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PythonRDD[4] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFile = sc.textFile(HDFS + 'hour_noheader.csv')\n",
    "data = dataFile.map(lambda line : line.strip().split(','))\n",
    "print(data.count())\n",
    "print(data.first())\n",
    "data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total length of categorical feature:57\n",
      "Feature vector length for categorical features: 57\n",
      "Feature vector length for numerical features: 4\n",
      "Total feature vector length: 61\n"
     ]
    }
   ],
   "source": [
    "# one-hot encoding for feature[2:10]\n",
    "length = sum([data.map(lambda x:x[i]).distinct().count() for i in xrange(2,10)])\n",
    "print('total length of categorical feature:{0}'.format(length))\n",
    "\n",
    "def get_mapping(rdd, idx):\n",
    "    return rdd.map(lambda x:x[idx]).distinct().zipWithIndex().collectAsMap()\n",
    "\n",
    "mappings = [get_mapping(data, i) for i in xrange(2,10)]\n",
    "\n",
    "cat_len = sum([len(m) for m in mappings])\n",
    "num_len = len(data.first()[11:15])\n",
    "total_len = cat_len + num_len\n",
    "\n",
    "print(\"Feature vector length for categorical features: %d\" % cat_len)\n",
    "print(\"Feature vector length for numerical features: %d\" % num_len)\n",
    "print(\"Total feature vector length: %d\" % total_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating feature vectors for the linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.      0.      0.      0.      0.      1.      0.      1.      0.      0.\n",
      "  0.      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "  0.      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "  0.      0.      0.      0.      0.      0.      1.      0.      0.      0.\n",
      "  0.      0.      0.      1.      0.      0.      0.      0.      0.      0.\n",
      "  1.      0.      1.      1.      0.      0.      0.      0.24    0.2879\n",
      "  0.81    0.    ]\n",
      "(16.0,[1.0,0.0,0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,1.0,1.0,0.0,0.0,0.0,0.24,0.2879,0.81,0.0])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# extract features and label and encapsulate as LabeledPoint\n",
    "def extract_features(record, mappings, cat_len):\n",
    "    cate_features = np.zeros(cat_len)\n",
    "    offset = 0\n",
    "    for i, val in enumerate(record[2:10]):\n",
    "        idx = offset + mappings[i][val]\n",
    "        cate_features[idx] = 1\n",
    "        offset += len(mappings[i])\n",
    "    num_features = np.array([float(field) for field in record[10:14]])\n",
    "    return np.concatenate((cate_features, num_features))\n",
    "\n",
    "print(extract_features(data.first(), mappings, cat_len))\n",
    "\n",
    "def extract_label(record):\n",
    "    return float(record[-1])\n",
    "\n",
    "processed_data = data.map(lambda x: LabeledPoint(extract_label(x), extract_features(x,mappings, cat_len)))\n",
    "print(processed_data.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating feature vectors for the decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16.0,[1.0,0.0,1.0,0.0,0.0,6.0,0.0,1.0,0.24,0.2879,0.81,0.0])\n"
     ]
    }
   ],
   "source": [
    "# decision tree models typically work on raw features \n",
    "# that is, it is not required to convert categorical features into a binary vector encoding; \n",
    "def extract_features_dt(record):\n",
    "    return np.array(map(float, record[2:14]))\n",
    "dt_data = data.map(lambda x:LabeledPoint(extract_label(x), extract_features_dt(x)))\n",
    "print(dt_data.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and using regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LinearRegressionWithSGD\n",
    "from pyspark.mllib.tree import DecisionTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.toree.interpreter.broker.BrokerException\n",
       "Message: Traceback (most recent call last):\n",
       "  File \"/tmp/kernel-PySpark-08b9ceba-bf5a-429b-9f45-231454d0ee26/pyspark_runner.py\", line 194, in <module>\n",
       "    eval(compiled_code)\n",
       "  File \"<string>\", line 1, in <module>\n",
       "NameError: name 'LinearRegression' is not defined\n",
       "\n",
       "StackTrace: org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:163)\n",
       "org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:163)\n",
       "scala.Option.foreach(Option.scala:257)\n",
       "org.apache.toree.interpreter.broker.BrokerState.markFailure(BrokerState.scala:162)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:498)\n",
       "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
       "py4j.Gateway.invoke(Gateway.java:280)\n",
       "py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "py4j.GatewayConnection.run(GatewayConnection.java:214)\n",
       "java.lang.Thread.run(Thread.java:745)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "help(LinearRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method trainRegressor in module pyspark.mllib.tree:\n",
      "\n",
      "trainRegressor(cls, data, categoricalFeaturesInfo, impurity='variance', maxDepth=5, maxBins=32, minInstancesPerNode=1, minInfoGain=0.0) method of __builtin__.type instance\n",
      "    Train a decision tree model for regression.\n",
      "    \n",
      "    :param data:\n",
      "      Training data: RDD of LabeledPoint. Labels are real numbers.\n",
      "    :param categoricalFeaturesInfo:\n",
      "      Map storing arity of categorical features. An entry (n -> k)\n",
      "      indicates that feature n is categorical with k categories\n",
      "      indexed from 0: {0, 1, ..., k-1}.\n",
      "    :param impurity:\n",
      "      Criterion used for information gain calculation.\n",
      "      The only supported value for regression is \"variance\".\n",
      "      (default: \"variance\")\n",
      "    :param maxDepth:\n",
      "      Maximum depth of tree (e.g. depth 0 means 1 leaf node, depth 1\n",
      "      means 1 internal node + 2 leaf nodes).\n",
      "      (default: 5)\n",
      "    :param maxBins:\n",
      "      Number of bins used for finding splits at each node.\n",
      "      (default: 32)\n",
      "    :param minInstancesPerNode:\n",
      "      Minimum number of instances required at child nodes to create\n",
      "      the parent split.\n",
      "      (default: 1)\n",
      "    :param minInfoGain:\n",
      "      Minimum info gain required to create a split.\n",
      "      (default: 0.0)\n",
      "    :return:\n",
      "      DecisionTreeModel.\n",
      "    \n",
      "    Example usage:\n",
      "    \n",
      "    >>> from pyspark.mllib.regression import LabeledPoint\n",
      "    >>> from pyspark.mllib.tree import DecisionTree\n",
      "    >>> from pyspark.mllib.linalg import SparseVector\n",
      "    >>>\n",
      "    >>> sparse_data = [\n",
      "    ...     LabeledPoint(0.0, SparseVector(2, {0: 0.0})),\n",
      "    ...     LabeledPoint(1.0, SparseVector(2, {1: 1.0})),\n",
      "    ...     LabeledPoint(0.0, SparseVector(2, {0: 0.0})),\n",
      "    ...     LabeledPoint(1.0, SparseVector(2, {1: 2.0}))\n",
      "    ... ]\n",
      "    >>>\n",
      "    >>> model = DecisionTree.trainRegressor(sc.parallelize(sparse_data), {})\n",
      "    >>> model.predict(SparseVector(2, {1: 1.0}))\n",
      "    1.0\n",
      "    >>> model.predict(SparseVector(2, {1: 0.0}))\n",
      "    0.0\n",
      "    >>> rdd = sc.parallelize([[0.0, 1.0], [0.0, 0.0]])\n",
      "    >>> model.predict(rdd).collect()\n",
      "    [1.0, 0.0]\n",
      "    \n",
      "    .. versionadded:: 1.1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(DecisionTree.trainRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/mllib/regression.py:281: UserWarning: Deprecated in 2.0.0. Use ml.regression.LinearRegression.\n",
      "  warnings.warn(\"Deprecated in 2.0.0. Use ml.regression.LinearRegression.\")\n",
      "(16.0, 72.447168855043827)\n"
     ]
    }
   ],
   "source": [
    "# linear regression\n",
    "linear_model = LinearRegressionWithSGD.train(processed_data, iterations=1000, step=0.1, intercept=False)\n",
    "true_vs_predict = processed_data.map(lambda p: (p.label, linear_model.predict(p.features)))\n",
    "print(true_vs_predict.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16.0, 54.913223140495866)\n"
     ]
    }
   ],
   "source": [
    "# decision tree\n",
    "dt_model = DecisionTree.trainRegressor(dt_data, {})\n",
    "preds = dt_model.predict(dt_data.map(lambda x : x.features))\n",
    "true_vs_predict_dt = dt_data.map(lambda p:p.label).zip(preds)\n",
    "print(true_vs_predict_dt.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression:\n",
      "Mean squared error:22905.9187726\n",
      "Mean absolute error:114.67503594\n",
      "Decision Tree:\n",
      "Mean squared error:11611.4859995\n",
      "Mean absolute error:71.1501878649\n"
     ]
    }
   ],
   "source": [
    "def squared_error(record):\n",
    "    return (record[0] - record[1])**2\n",
    "\n",
    "def abs_error(record):\n",
    "    return abs(record[0] - record[1])\n",
    "\n",
    "mean_squared_error = true_vs_predict.map(lambda r:squared_error(r)).mean()\n",
    "mean_abs_error = true_vs_predict.map(lambda r:abs_error(r)).mean()\n",
    "print('Linear Regression:\\nMean squared error:{0}\\nMean absolute error:{1}'.format(mean_squared_error, mean_abs_error))\n",
    "\n",
    "mean_squared_error_dt = true_vs_predict_dt.map(lambda r:squared_error(r)).mean()\n",
    "mean_abs_error_dt = true_vs_predict_dt.map(lambda r:abs_error(r)).mean()\n",
    "print('Decision Tree:\\nMean squared error:{0}\\nMean absolute error:{1}'.format(mean_squared_error_dt, mean_abs_error_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving model performance and tuning parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# many machine learning models, including linear models\n",
    "# make assumptions regarding the distribution of the input data as well as target variables.\n",
    "# In particular, linear regression assumes a normal distribution\n",
    "\n",
    "# when target variable do not follow a normal distribution\n",
    "# take the logarithm of the target value instead of the raw value. \n",
    "# This is often referred to as log-transforming the target variable\n",
    "log_transformed_data = processed_data.map(lambda p: LabeledPoint(np.log(p.label), p.features))\n",
    "\n",
    "\n",
    "# A second type of transformation that is useful in the case of target values that do not\n",
    "# take on negative values and, in addition, might take on a very wide range of values,\n",
    "# is to take the square root of the variable\n",
    "square_transformed_data = dt_data.map(lambda p: LabeledPoint(np.sqrt(p.label), p.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression after log transform:\n",
      "Mean squared error:1.73824417498\n",
      "Mean absolute error:1.03879865848\n"
     ]
    }
   ],
   "source": [
    "# performance of linear regression after log transform of target variable\n",
    "linear_model_log =  LinearRegressionWithSGD.train(log_transformed_data, iterations=1000, step=0.1, intercept=False)\n",
    "true_vs_predict_log = log_transformed_data.map(lambda p: (p.label, linear_model_log.predict(p.features)))\n",
    "mean_squared_error_log = true_vs_predict_log.map(lambda r:squared_error(r)).mean()\n",
    "mean_abs_error_log = true_vs_predict_log.map(lambda r:abs_error(r)).mean()\n",
    "print('Linear Regression after log transform:\\nMean squared error:{0}\\nMean absolute error:{1}'.format(mean_squared_error_log, mean_abs_error_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree after log transformation:\n",
      "Mean squared error:11.1814998545\n",
      "Mean absolute error:2.43810743015\n"
     ]
    }
   ],
   "source": [
    "# performance of decision tree after log transform of target variable\n",
    "dt_model_log = DecisionTree.trainRegressor(square_transformed_data, {})\n",
    "preds = dt_model_log.predict(square_transformed_data.map(lambda x : x.features))\n",
    "true_vs_predict_dt_log = square_transformed_data.map(lambda p:p.label).zip(preds)\n",
    "mean_squared_error_dt_log = true_vs_predict_dt_log.map(lambda r:squared_error(r)).mean()\n",
    "mean_abs_error_dt_log = true_vs_predict_dt_log.map(lambda r:abs_error(r)).mean()\n",
    "print('Decision Tree after log transformation:\\nMean squared error:{0}\\nMean absolute error:{1}'.format(mean_squared_error_dt_log, mean_abs_error_dt_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating training and testing sets to evaluate parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of train data: 13915\n",
      "number of test data: 3464\n"
     ]
    }
   ],
   "source": [
    "# Spark's Python API does not yet provide the randomSplit convenience method that is available in Scala\n",
    "# Hence, we will need to create a training and test dataset manually\n",
    "\n",
    "# We can achieve this using the sample method to take a random sample for our test set\n",
    "# followed by using the subtractByKey method, which takes care of \n",
    "# returning the elements in one RDD where the keys do not overlap with the other RDD\n",
    "\n",
    "# subtractByKey, as the name suggests, works on the keys of the RDD elements that consist of key-value pairs\n",
    "# Therefore, here we will use zipWithIndex on our RDD of extracted training examples. \n",
    "# This creates an RDD of (LabeledPoint, index) pairs\n",
    "\n",
    "test_data_ratio = 0.2\n",
    "index_log_transformed_data = log_transformed_data.zipWithIndex().map(lambda (k,v) :(v, k))\n",
    "test_data = index_log_transformed_data.sample(withReplacement = False, fraction = test_data_ratio, seed = 123)\n",
    "train_data = index_log_transformed_data.subtractByKey(test_data)\n",
    "\n",
    "train_data = train_data.map(lambda (idx, p): p)\n",
    "test_data = test_data.map(lambda (idx, p):p)\n",
    "print('number of train data: {0}\\nnumber of test data: {1}'.format(train_data.count(), test_data.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The impact of parameter settings for linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(train, test, iterations, step, regParam, regType, intercept):\n",
    "    model = LinearRegressionWithSGD.train(train, iterations, step,\\\n",
    "                                          regParam=regParam, regType=regType, intercept=intercept)\n",
    "    tp = test.map(lambda p: (p.label, model.predict(p.features)))\n",
    "    mse = tp.map(lambda p: squared_error(p)).mean()\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 5, 10, 20, 50, 100]\n",
      "[20.845951702978386, 17.747592473707126, 15.645164342260253, 13.109441839169357, 9.3694050200682746, 6.6405468137387489]\n"
     ]
    }
   ],
   "source": [
    "# iterations\n",
    "# store all different values needed to be evaluated in a list\n",
    "params = [1, 5, 10, 20, 50, 100]\n",
    "metrics = [evaluate(train_data, test_data, param, 0.01, 0.0, 'l2', False) for param in params]\n",
    "print(params)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01, 0.025, 0.05, 0.1, 1.0]\n",
      "[15.645164342260253, 9.3171259065620831, 4.4756827986761412, 2.2509450032709895, 1.4501825618154074]\n"
     ]
    }
   ],
   "source": [
    "# Step size\n",
    "params = [0.01, 0.025, 0.05, 0.1, 1.0]\n",
    "metrics = [evaluate(train_data, test_data, 10, param, 0.0, 'l2', False) for param in params]\n",
    "print(params)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.01, 0.1, 1.0, 5.0, 10.0, 20.0]\n",
      "[2.2509450032709895, 2.2573695890496688, 2.3189589425597323, 3.2151109211262123, 8.357292719160256, 12.46097422684992, 16.220273970916246]\n"
     ]
    }
   ],
   "source": [
    "# different levels of L2 regularization \n",
    "params = [0.0, 0.01, 0.1, 1.0, 5.0, 10.0, 20.0]\n",
    "metrics = [evaluate(train_data, test_data, 10, 0.1, param, 'l2', False) for param in params]\n",
    "print(params)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False, True]\n",
      "[3.2151109211262123, 2.6797426251036804]\n"
     ]
    }
   ],
   "source": [
    "# effect of adding an intercept term to the model\n",
    "# always add the intercept\n",
    "params = [False, True]\n",
    "metrics = [evaluate(train_data, test_data, 10, 0.1, 1.0, 'l2', param) for param in params]\n",
    "print(params)\n",
    "print(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - PySpark",
   "language": "python",
   "name": "apache_toree_pyspark"
  },
  "language_info": {
   "codemirror_mode": "text/x-ipython",
   "file_extension": ".py",
   "mimetype": "text/x-ipython",
   "name": "python",
   "pygments_lexer": "python",
   "version": "2.7.11\n"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
