{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SparkContext is already defined as sc\n",
    "HDFS = 'hdfs://scut0:9000/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting features from the LFW dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the face data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Spark provides us with a way to read text files and custom Hadoop input data sources. However, there is no built-in functionality to allow us to read images\n",
    "- Spark provides a method called wholeTextFiles, which allows us to operate on entire files at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1054\n"
     ]
    }
   ],
   "source": [
    "# binaryFiles returns an RDD that contains key-value pairs,\n",
    "# where the key is the path of the file while the value is the content of binary file\n",
    "pairRDD = sc.binaryFiles(HDFS+'/lfw/*')\n",
    "print(pairRDD.count())\n",
    "# print(pairRDD.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting facial images as vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For many image-processing and machine learning tasks related to images\n",
    "it is common to operate on grayscale images. \n",
    "\n",
    "\n",
    "- We will do this here by converting the color images to grayscale frst.\n",
    "- then resize the images to 100 x 100 pixels(original 250 X 250) and flatten it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1054\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "def img2Vector(imgStr):\n",
    "    resizedSize = (100, 100)\n",
    "    npArr = np.fromstring(imgStr, np.uint8)\n",
    "    npImg = cv2.imdecode(npArr, cv2.IMREAD_COLOR)\n",
    "    grayImg = cv2.cvtColor(npImg, cv2.COLOR_BGR2GRAY)\n",
    "    return cv2.resize(grayImg, resizedSize).flatten()\n",
    "\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "imgVectors = pairRDD.map(lambda (path, imgStr): Vectors.dense(img2Vector(imgStr)))\n",
    "print(imgVectors.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.mllib.linalg.DenseVector'>\n"
     ]
    }
   ],
   "source": [
    "imgVectors.cache\n",
    "print(type(imgVectors.first()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a common practice to standardize input data prior to running dimensionality reduction models, in particular for PCA.\n",
    "we will do this using the built-in `StandardScaler` provided by MLlib's feature package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import StandardScaler\n",
    "scaler = StandardScaler(withMean = True, withStd = False).fit(imgVectors)\n",
    "scaledImgVectors = scaler.transform(imgVectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a dimensionality reduction model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality reduction models in MLlib require vectors as inputs. However,\n",
    "unlike clustering that operated on an RDD[Vector], PCA and SVD computations are\n",
    "provided as methods on a distributed RowMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running PCA on the LFW dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "matrix = RowMatrix(scaledImgVectors)\n",
    "# PCA is not supported in pyspark before spark 2.1.0\n",
    "# pc = matrix.computePrincipalComponents(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Eigenfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the above the PCA is not available in pyspark, here we just show the result that the book provide\n",
    "below are the top 10 engenfaces extracted with PCA\n",
    "\n",
    "![](http://static.zybuluo.com/WuLiangchao/ull2ex1a454wcndk07tj5pgg/eigenfaces.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above [eigenfaces](https://en.wikipedia.org/wiki/Eigenface) are actually the eigenvectors from PCA, we just visualize them as images\n",
    "\n",
    "Looking at the preceding images, we can see that the PCA model has effectively extracted recurring patterns of variation, which represent various features of the facial images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a dimensionality reduction model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall purpose of using dimensionality reduction is to create a more compact representation of the data that still captures the important features and variability\n",
    "in the raw dataset.\n",
    "\n",
    "To do this, we need to use a trained model to transform our raw data by projecting it into the new, lower-dimensional space represented by the principal components.\n",
    "\n",
    "Transformation can be easily implemented by a matrix multiplication of the image matrix with the matrix of principal components,but as mentioned above, PCA doesn't work here, so we just comment the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# projected = matrix.multiply(pc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The relationship between PCA and SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a close relationship between PCA and SVD. In fact, we can recover the same principal components and also apply the same\n",
    "projection into the space of principal components using SVD\n",
    "\n",
    "In our example, **the right singular vectors(svd.V below) derived from computing the SVD will be equivalent to the principal components** we have calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVD is not supported either\n",
    "\"\"\"\n",
    "svd = matrix.computeSVD(10, computeU=True)\n",
    "U = svd.U       # The U factor is a RowMatrix.\n",
    "s = svd.s       # The singular values are stored in a local dense vector.\n",
    "V = svd.V       # The V factor is a local dense matrix\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The other relationship that holds is that the multiplication of the matrix U and vector\n",
    "s (or, strictly speaking, the diagonal matrix s) is equivalent to the PCA projection of\n",
    "our original image data into the space of the top 10 principal components.**\n",
    "\n",
    "More details about SVD and PCA can be obtained here: https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - PySpark",
   "language": "python",
   "name": "apache_toree_pyspark"
  },
  "language_info": {
   "codemirror_mode": "text/x-ipython",
   "file_extension": ".py",
   "mimetype": "text/x-ipython",
   "name": "python",
   "pygments_lexer": "python",
   "version": "2.7.11\n"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
